{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammed-alaa40123/Guardian-Angel/blob/main/train_yolov8_object_detection_on_custom_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe9vkEvFABbN"
      },
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# How to Train YOLOv8 Object Detection on a Custom Dataset\n",
        "\n",
        "---\n",
        "\n",
        "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset)\n",
        "[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/wuZtUMEiKWY)\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/ultralytics)\n",
        "\n",
        "Ultralytics YOLOv8 is the latest version of the YOLO (You Only Look Once) object detection and image segmentation model developed by Ultralytics. The YOLOv8 model is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and image segmentation tasks. It can be trained on large datasets and is capable of running on a variety of hardware platforms, from CPUs to GPUs.\n",
        "\n",
        "## ‚ö†Ô∏è Disclaimer\n",
        "\n",
        "YOLOv8 is still under heavy development. Breaking changes are being introduced almost weekly. We strive to make our YOLOv8 notebooks work with the latest version of the library. Last tests took place on **03.01.2024** with version **YOLOv8.0.196**.\n",
        "\n",
        "If you notice that our notebook behaves incorrectly - especially if you experience errors that prevent you from going through the tutorial - don't hesitate! Let us know and open an [issue](https://github.com/roboflow/notebooks/issues) on the Roboflow Notebooks repository.\n",
        "\n",
        "## Accompanying Blog Post\n",
        "\n",
        "We recommend that you follow along in this notebook while reading the blog post on how to train YOLOv8 Object Detection, concurrently.\n",
        "\n",
        "## Pro Tip: Use GPU Acceleration\n",
        "\n",
        "If you are running this notebook in Google Colab, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. This will ensure your notebook uses a GPU, which will significantly speed up model training times.\n",
        "\n",
        "## Steps in this Tutorial\n",
        "\n",
        "In this tutorial, we are going to cover:\n",
        "\n",
        "- Before you start\n",
        "- Install YOLOv8\n",
        "- CLI Basics\n",
        "- Inference with Pre-trained COCO Model\n",
        "- Roboflow Universe\n",
        "- Preparing a custom dataset\n",
        "- Custom Training\n",
        "- Validate Custom Model\n",
        "- Inference with Custom Model\n",
        "\n",
        "**Let's begin!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyRdDYkqAKN4"
      },
      "source": [
        "## Before you start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8cDtxLIBHgQ",
        "outputId": "1ca7f9ec-0682-4c9b-cad3-00ad7e005f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan 28 11:11:44 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjpPg4mGKc1v",
        "outputId": "bdb6d169-8711-4bcc-c752-86dcc74fc718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C3EO_2zNChu"
      },
      "source": [
        "## Install YOLOv8\n",
        "\n",
        "‚ö†Ô∏è YOLOv8 is still under heavy development. Breaking changes are being introduced almost weekly. We strive to make our YOLOv8 notebooks work with the latest version of the library. Last tests took place on **03.01.2024** with version **YOLOv8.0.196**.\n",
        "\n",
        "If you notice that our notebook behaves incorrectly - especially if you experience errors that prevent you from going through the tutorial - don't hesitate! Let us know and open an [issue](https://github.com/roboflow/notebooks/issues) on the Roboflow Notebooks repository.\n",
        "\n",
        "YOLOv8 can be installed in two ways‚Ää-‚Ääfrom the source and via pip. This is because it is the first iteration of YOLO to have an official package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdSMcABDNKW-",
        "outputId": "9931174a-7ce5-4420-b1f3-ed7da219c079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.196 üöÄ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 26.6/78.2 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# Pip install method (recommended)\n",
        "\n",
        "!pip install ultralytics==8.0.196\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVvaIYEEPOty"
      },
      "outputs": [],
      "source": [
        "# Git clone method (for development)\n",
        "\n",
        "# %cd {HOME}\n",
        "# !git clone github.com/ultralytics/ultralytics\n",
        "# %cd {HOME}/ultralytics\n",
        "# !pip install -e .\n",
        "\n",
        "# from IPython import display\n",
        "# display.clear_output()\n",
        "\n",
        "# import ultralytics\n",
        "# ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VOEYrlBoP9-E"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "from IPython.display import display, Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnnZSm5OQfPQ"
      },
      "source": [
        "## CLI Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K33S7zlkQku0"
      },
      "source": [
        "If you want to train, validate or run inference on models and don't need to make any modifications to the code, using YOLO command line interface is the easiest way to get started. Read more about CLI in [Ultralytics YOLO Docs](https://docs.ultralytics.com/usage/cli/).\n",
        "\n",
        "```\n",
        "yolo task=detect    mode=train    model=yolov8n.yaml      args...\n",
        "          classify       predict        yolov8n-cls.yaml  args...\n",
        "          segment        val            yolov8n-seg.yaml  args...\n",
        "                         export         yolov8n.pt        format=onnx  args...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5RGYA6sPgEd"
      },
      "source": [
        "## Inference with Pre-trained COCO Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT1qD4toTTw0"
      },
      "source": [
        "### üíª CLI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaE1kLS8R4CV"
      },
      "source": [
        "`yolo mode=predict` runs YOLOv8 inference on a variety of sources, downloading models automatically from the latest YOLOv8 release, and saving results to `runs/predict`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDbMt_M6PiXb",
        "outputId": "39482f68-c158-4665-8bd0-6e21e4af3f0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n",
            "100% 6.23M/6.23M [00:00<00:00, 80.2MB/s]\n",
            "Ultralytics YOLOv8.0.196 üöÄ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
            "\n",
            "Downloading https://media.roboflow.com/notebooks/examples/dog.jpeg to 'dog.jpeg'...\n",
            "100% 104k/104k [00:00<00:00, 62.2MB/s]\n",
            "WARNING ‚ö†Ô∏è NMS time limit 0.550s exceeded\n",
            "image 1/1 /content/dog.jpeg: 640x384 1 person, 1 car, 1 dog, 112.6ms\n",
            "Speed: 12.9ms preprocess, 112.6ms inference, 699.9ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
            "üí° Learn more at https://docs.ultralytics.com/modes/predict\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "!yolo task=detect mode=predict model=yolov8n.pt conf=0.25 source='https://media.roboflow.com/notebooks/examples/dog.jpeg' save=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "LyopYpK1TQrB",
        "outputId": "ebb88d15-051b-4195-9e2b-b34cd2553ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Image' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d022ce02db12>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{HOME}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'runs/detect/predict/dog.jpeg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "Image(filename='runs/detect/predict/dog.jpeg', height=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFMBYQtMVL-B"
      },
      "source": [
        "### üêç Python SDK\n",
        "\n",
        "The simplest way of simply using YOLOv8 directly in a Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx9NWF-sVN6Y",
        "outputId": "f8b8e341-ae74-4b33-92ea-0e0c10a63902"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.20 üöÄ Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15110MiB)\n",
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
            "Found https://media.roboflow.com/notebooks/examples/dog.jpeg locally at dog.jpeg\n"
          ]
        }
      ],
      "source": [
        "model = YOLO(f'{HOME}/yolov8n.pt')\n",
        "results = model.predict(source='https://media.roboflow.com/notebooks/examples/dog.jpeg', conf=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAi4PvrItTCf",
        "outputId": "3a1a1c21-be10-437f-aa14-4995d5321789"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[   0.,  314.,  625., 1278.],\n",
              "        [  55.,  250.,  648., 1266.],\n",
              "        [ 633.,  720.,  701.,  786.]], device='cuda:0')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results[0].boxes.xyxy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqT2M01K1LUb",
        "outputId": "ac8d0988-8be7-4fec-c62b-2cd8fe9b5371"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.72712, 0.29066, 0.28456], device='cuda:0')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results[0].boxes.conf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKIwJ5yw1PMb",
        "outputId": "ee27ea55-240f-43fd-d9a3-e8b8a73149fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 0., 16.,  2.], device='cuda:0')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results[0].boxes.cls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2Xtaekw3271"
      },
      "source": [
        "## Roboflow Universe\n",
        "\n",
        "Need data for your project? Before spending time on annotating, check out Roboflow Universe, a repository of more than 110,000 open-source datasets that you can use in your projects. You'll find datasets containing everything from annotated cracks in concrete to plant images with disease annotations.\n",
        "\n",
        "\n",
        "[![Roboflow Universe](https://media.roboflow.com/notebooks/template/uni-banner-frame.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672878480290)](https://universe.roboflow.com/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JHICVjZbVKn"
      },
      "source": [
        "## Preparing a custom¬†dataset\n",
        "\n",
        "Building a custom dataset can be a painful process. It might take dozens or even hundreds of hours to collect images, label them, and export them in the proper format. Fortunately, Roboflow makes this process as straightforward and fast as possible. Let me show you how!\n",
        "\n",
        "### Step 1: Creating project\n",
        "\n",
        "Before you start, you need to create a Roboflow [account](https://app.roboflow.com/login). Once you do that, you can create a new project in the Roboflow [dashboard](https://app.roboflow.com/). Keep in mind to choose the right project type. In our case, Object Detection.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/creating-project.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672929799852\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "### Step 2: Uploading images\n",
        "\n",
        "Next, add the data to your newly created project. You can do it via API or through our [web interface](https://docs.roboflow.com/adding-data/object-detection).\n",
        "\n",
        "If you drag and drop a directory with a dataset in a supported format, the Roboflow dashboard will automatically read the images and annotations together.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/uploading-images.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672929808290\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "### Step 3: Labeling\n",
        "\n",
        "If you only have images, you can label them in [Roboflow Annotate](https://docs.roboflow.com/annotate).\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://user-images.githubusercontent.com/26109316/210901980-04861efd-dfc0-4a01-9373-13a36b5e1df4.gif\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "### Step 4: Generate new dataset version\n",
        "\n",
        "Now that we have our images and annotations added, we can Generate a Dataset Version. When Generating a Version, you may elect to add preprocessing and augmentations. This step is completely optional, however, it can allow you to significantly improve the robustness of your model.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/generate-new-version.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1673003597834\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "### Step 5: Exporting dataset\n",
        "\n",
        "Once the dataset version is generated, we have a hosted dataset we can load directly into our notebook for easy training. Click `Export` and select the `YOLO v5 PyTorch` dataset format.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/export.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672943313709\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSd93ZJzZZKt",
        "outputId": "d5ba9535-022d-4c23-a476-16eafd55afb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‚Äò/content/datasets‚Äô: File exists\n",
            "/content/datasets\n",
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.17)\n",
            "Requirement already satisfied: certifi==2023.7.22 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2023.7.22)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.23.5)\n",
            "Requirement already satisfied: opencv-python-headless==4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.8.0.74)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: supervision in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.18.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.4.27)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.47.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from supervision->roboflow) (0.7.1)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from supervision->roboflow) (1.11.4)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        }
      ],
      "source": [
        "!mkdir {HOME}/datasets\n",
        "%cd {HOME}/datasets\n",
        "\n",
        "!pip install roboflow --quiet\n",
        "\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"zlM2sfuWgetVEB9qCkaM\")\n",
        "project = rf.workspace(\"projects-cyytt\").project(\"child-monitoring-9rzyo\")\n",
        "dataset = project.version(11).download(\"yolov8\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUjFBKKqXa-u"
      },
      "source": [
        "## Custom Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2YkphuiaE7_",
        "outputId": "d5803e1c-d0cb-4795-c36c-7c3542dab41c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt to 'yolov8s.pt'...\n",
            "100% 21.5M/21.5M [00:00<00:00, 55.1MB/s]\n",
            "New https://pypi.org/project/ultralytics/8.1.6 available üòÉ Update with 'pip install -U ultralytics'\n",
            "Ultralytics YOLOv8.0.196 üöÄ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/content/datasets/child-monitoring-11/data.yaml, epochs=100, patience=50, batch=16, imgsz=800, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100% 755k/755k [00:00<00:00, 5.06MB/s]\n",
            "2024-01-28 17:08:51.014982: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-28 17:08:51.015042: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-28 17:08:51.016373: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2117596  ultralytics.nn.modules.head.Detect           [4, [128, 256, 512]]          \n",
            "Model summary: 225 layers, 11137148 parameters, 11137132 gradients, 28.7 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n",
            "100% 6.23M/6.23M [00:00<00:00, 23.3MB/s]\n",
            "WARNING ‚ö†Ô∏è NMS time limit 0.550s exceeded\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/child-monitoring-11/train/labels... 1555 images, 0 backgrounds, 0 corrupt: 100% 1555/1555 [00:00<00:00, 2108.43it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/2_16_jpg.rf.7e738906f9d87df790198bbf40c52252.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/2_16_jpg.rf.d844e7e13b1038ac11ca7d3461d1582f.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/2_16_jpg.rf.dcb2bea163d8d20daa90eecf2c89e32d.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/2_19_jpg.rf.41b5c07803fc5b28b0144cbabd53a205.jpg: 5 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/2_19_jpg.rf.64a7bcdd1878afaa6000f0a5c321261e.jpg: 5 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/2_19_jpg.rf.9b64114d4118ba1915aa877e5c4b6593.jpg: 5 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/3_4_jpg.rf.07750abf391ac2a22546770ae6af0b65.jpg: 3 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/3_4_jpg.rf.ce70e3d470acac897ab32bf72fb1dd19.jpg: 3 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/3_4_jpg.rf.f10968e276df2dc373873f24a192b3d3.jpg: 3 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/6_20_jpg.rf.2aacb17788ce1b1c44e0ea57256daa4d.jpg: 3 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/6_20_jpg.rf.a658cf5723ae27a0541bbd002d2aeee6.jpg: 3 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/6_20_jpg.rf.b517f484ce85c98007c3b0bfda4af35b.jpg: 3 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/G1347__160__0-25__0-29__fix_anonymized__frame-14_png_jpg.rf.0fcb02cdd1433e276fb3c3c96e3a84a5.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/G1347__160__0-25__0-29__fix_anonymized__frame-14_png_jpg.rf.62f135acc1cac30459e8b9947ad54cc4.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/G1347__160__0-25__0-29__fix_anonymized__frame-14_png_jpg.rf.b88f8d6faa186a44ff1724620486db78.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/G1658__170__0-15__0-17__fix_anonymized__frame-19_png_jpg.rf.8bdcf505a6f2aa0834d2aff5e9adb006.jpg: 7 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/G1658__170__0-15__0-17__fix_anonymized__frame-19_png_jpg.rf.92d9b85d1cdcae00b851e7ea5f417089.jpg: 7 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/G1658__170__0-15__0-17__fix_anonymized__frame-19_png_jpg.rf.be32c75a19b080afc73e912d3cc12d51.jpg: 7 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/IMG-20240123-WA0041_jpg.rf.3ffa3bb9a9c584034d17ca262cc92aaa.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/IMG-20240123-WA0041_jpg.rf.e031e8f9593b78b6755c7b56eeac45e8.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/datasets/child-monitoring-11/train/images/IMG-20240123-WA0041_jpg.rf.e2e17bb693b0f9b374bbfca90e1e23f1.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/child-monitoring-11/train/labels.cache\n",
            "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 149, len(boxes) = 14038. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/child-monitoring-11/valid/labels... 67 images, 0 backgrounds, 0 corrupt: 100% 67/67 [00:00<00:00, 1656.41it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/child-monitoring-11/valid/labels.cache\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 800 train, 800 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      1/100      6.34G      1.849      2.098      1.735         27        800: 100% 98/98 [00:59<00:00,  1.66it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:02<00:00,  1.02it/s]\n",
            "                   all         67        555      0.395      0.478      0.381      0.164\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      2/100      6.58G      1.809      1.814      1.738         13        800: 100% 98/98 [00:53<00:00,  1.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.01it/s]\n",
            "                   all         67        555      0.312      0.289      0.237     0.0943\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      3/100      6.58G      1.814      1.863      1.764         52        800: 100% 98/98 [00:54<00:00,  1.81it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.08it/s]\n",
            "                   all         67        555       0.28      0.289      0.241     0.0849\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      4/100      6.37G      1.816      1.817      1.769         36        800: 100% 98/98 [00:52<00:00,  1.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.55it/s]\n",
            "                   all         67        555      0.399      0.462      0.377      0.152\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      5/100      6.61G      1.781      1.764      1.758         16        800: 100% 98/98 [00:53<00:00,  1.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.84it/s]\n",
            "                   all         67        555      0.413      0.363      0.341      0.127\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      6/100       6.6G      1.752      1.692      1.724         38        800: 100% 98/98 [00:53<00:00,  1.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.47it/s]\n",
            "                   all         67        555      0.426      0.443      0.397      0.158\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      7/100      6.58G      1.717      1.621      1.695         41        800: 100% 98/98 [00:52<00:00,  1.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.19it/s]\n",
            "                   all         67        555      0.407      0.415      0.361      0.142\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      8/100      6.57G        1.7      1.593      1.693         22        800: 100% 98/98 [00:55<00:00,  1.78it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.38it/s]\n",
            "                   all         67        555      0.423      0.402      0.396      0.171\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      9/100      6.49G      1.682      1.512      1.668         55        800: 100% 98/98 [00:52<00:00,  1.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.92it/s]\n",
            "                   all         67        555      0.372      0.472      0.405      0.171\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     10/100      6.56G      1.657      1.498      1.651         86        800: 100% 98/98 [00:52<00:00,  1.87it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.87it/s]\n",
            "                   all         67        555      0.514       0.42       0.44      0.187\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     11/100      6.61G      1.622       1.45      1.631         44        800: 100% 98/98 [00:51<00:00,  1.91it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.14it/s]\n",
            "                   all         67        555      0.475      0.499      0.456      0.203\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     12/100      6.57G      1.611       1.41      1.624         43        800: 100% 98/98 [00:53<00:00,  1.85it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.73it/s]\n",
            "                   all         67        555      0.446      0.484       0.45      0.203\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     13/100      6.63G      1.586      1.363      1.606         67        800: 100% 98/98 [00:53<00:00,  1.82it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.06it/s]\n",
            "                   all         67        555      0.455      0.442      0.427      0.194\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     14/100      6.62G      1.577      1.353      1.601         24        800: 100% 98/98 [00:53<00:00,  1.85it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.58it/s]\n",
            "                   all         67        555       0.54      0.487      0.467      0.217\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     15/100      6.61G      1.557      1.313      1.568         61        800: 100% 98/98 [00:53<00:00,  1.82it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.57it/s]\n",
            "                   all         67        555      0.566      0.428      0.446      0.208\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     16/100      6.53G      1.537      1.271      1.567         21        800: 100% 98/98 [00:52<00:00,  1.88it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.52it/s]\n",
            "                   all         67        555      0.617      0.456      0.487      0.221\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     17/100       6.6G      1.529       1.22      1.543         46        800: 100% 98/98 [00:51<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.10it/s]\n",
            "                   all         67        555      0.533      0.499       0.47      0.201\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     18/100      6.61G      1.503      1.216       1.54         23        800: 100% 98/98 [00:54<00:00,  1.81it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.17it/s]\n",
            "                   all         67        555      0.602      0.478       0.48       0.22\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     19/100      6.62G      1.489      1.193      1.516         55        800: 100% 98/98 [00:52<00:00,  1.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.19it/s]\n",
            "                   all         67        555      0.572      0.479      0.487      0.225\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     20/100       6.6G      1.484      1.174      1.521         30        800: 100% 98/98 [00:53<00:00,  1.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.06it/s]\n",
            "                   all         67        555      0.599      0.471      0.463      0.206\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     21/100      6.61G      1.458      1.127      1.502         95        800: 100% 98/98 [00:54<00:00,  1.81it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.38it/s]\n",
            "                   all         67        555      0.529      0.457      0.453      0.193\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     22/100      6.61G      1.451       1.12      1.488         55        800: 100% 98/98 [00:51<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.78it/s]\n",
            "                   all         67        555      0.538      0.422      0.464      0.214\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     23/100      6.68G      1.421      1.071      1.469         25        800: 100% 98/98 [00:53<00:00,  1.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.71it/s]\n",
            "                   all         67        555      0.573      0.468      0.482      0.223\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     24/100      6.65G      1.411      1.075      1.461         62        800: 100% 98/98 [00:51<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.22it/s]\n",
            "                   all         67        555      0.555      0.492        0.5      0.225\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     25/100      6.59G      1.397      1.041      1.451         34        800: 100% 98/98 [00:53<00:00,  1.82it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.49it/s]\n",
            "                   all         67        555      0.501      0.529      0.466      0.205\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     26/100      6.62G      1.374       1.03      1.438         35        800: 100% 98/98 [00:53<00:00,  1.82it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.40it/s]\n",
            "                   all         67        555      0.516      0.508      0.477      0.222\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     27/100      6.59G      1.371      1.014      1.423         51        800: 100% 98/98 [00:52<00:00,  1.85it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:02<00:00,  1.40it/s]\n",
            "                   all         67        555      0.483      0.499      0.454      0.201\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     28/100      6.61G       1.36      1.012      1.423         61        800: 100% 98/98 [00:53<00:00,  1.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.10it/s]\n",
            "                   all         67        555      0.575      0.464      0.465      0.192\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     29/100      6.64G      1.341     0.9698      1.409         41        800: 100% 98/98 [00:51<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:02<00:00,  1.40it/s]\n",
            "                   all         67        555      0.489      0.491      0.481      0.217\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     30/100      6.61G      1.348     0.9772      1.409         54        800: 100% 98/98 [00:51<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.19it/s]\n",
            "                   all         67        555      0.595       0.48      0.507      0.232\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     31/100      6.63G      1.312     0.9411      1.383         45        800: 100% 98/98 [00:53<00:00,  1.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.10it/s]\n",
            "                   all         67        555      0.546      0.516      0.473      0.228\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     32/100      6.59G      1.314     0.9447      1.383         20        800: 100% 98/98 [00:53<00:00,  1.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.86it/s]\n",
            "                   all         67        555      0.566      0.502      0.488      0.219\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     33/100      6.62G      1.279     0.9033      1.363         40        800: 100% 98/98 [00:53<00:00,  1.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.19it/s]\n",
            "                   all         67        555      0.585      0.501      0.482      0.232\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     34/100       6.6G      1.286     0.9148      1.364         72        800: 100% 98/98 [00:52<00:00,  1.85it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:02<00:00,  1.48it/s]\n",
            "                   all         67        555      0.553      0.474      0.488      0.235\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     35/100      6.61G      1.256     0.8945      1.345         83        800: 100% 98/98 [00:51<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.50it/s]\n",
            "                   all         67        555       0.55      0.467      0.489      0.233\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     36/100      6.58G      1.247     0.8791      1.335         34        800: 100% 98/98 [00:53<00:00,  1.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.36it/s]\n",
            "                   all         67        555      0.585      0.494      0.501      0.223\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     37/100      6.61G      1.242     0.8769      1.337         39        800: 100% 98/98 [00:53<00:00,  1.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.23it/s]\n",
            "                   all         67        555      0.574       0.51      0.492      0.225\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     38/100       6.6G      1.236      0.852      1.323         35        800: 100% 98/98 [00:53<00:00,  1.82it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.04it/s]\n",
            "                   all         67        555      0.602      0.474      0.482      0.226\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     39/100      6.62G      1.236     0.8458      1.322         17        800: 100% 98/98 [00:52<00:00,  1.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.88it/s]\n",
            "                   all         67        555      0.577      0.492      0.484      0.228\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     40/100      6.62G      1.216     0.8422      1.321         58        800: 100% 98/98 [00:53<00:00,  1.82it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.10it/s]\n",
            "                   all         67        555      0.594      0.517      0.499      0.231\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     41/100      6.77G      1.188     0.8116      1.295         99        800: 100% 98/98 [00:52<00:00,  1.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.12it/s]\n",
            "                   all         67        555      0.593       0.47      0.465      0.217\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     42/100      6.58G      1.211      0.831      1.313         31        800: 100% 98/98 [00:53<00:00,  1.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.07it/s]\n",
            "                   all         67        555      0.621      0.458      0.488      0.234\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     43/100      6.58G      1.194      0.812        1.3         47        800: 100% 98/98 [00:52<00:00,  1.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.56it/s]\n",
            "                   all         67        555      0.523      0.492      0.462      0.215\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     44/100      6.58G       1.18      0.794       1.28         59        800: 100% 98/98 [00:53<00:00,  1.85it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.57it/s]\n",
            "                   all         67        555       0.61      0.456      0.486      0.223\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     45/100      6.61G      1.169      0.798       1.28         67        800: 100% 98/98 [00:53<00:00,  1.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.04it/s]\n",
            "                   all         67        555      0.694      0.442      0.482      0.228\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     46/100      6.61G      1.149     0.7776       1.27         24        800: 100% 98/98 [00:53<00:00,  1.82it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.11it/s]\n",
            "                   all         67        555      0.575      0.501      0.498      0.234\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     47/100      6.62G       1.12     0.7533      1.253         48        800: 100% 98/98 [00:53<00:00,  1.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.99it/s]\n",
            "                   all         67        555      0.586      0.497        0.5      0.238\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     48/100      6.55G      1.125     0.7674      1.266        272        800:  62% 61/98 [00:32<00:25,  1.47it/s]"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "\n",
        "!yolo task=detect mode=train model=yolov8s.pt data={dataset.location}/data.yaml epochs=100 imgsz=800 plots=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MScstfHhArr",
        "outputId": "bb83d5ea-f571-413d-978e-04567f8ebbbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '{HOME}/runs/detect/train/': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls {HOME}/runs/detect/train/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "_J35i8Ofhjxa",
        "outputId": "eac97810-fbcc-4cb7-d6be-6952812e2d4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/runs/detect/train/confusion_matrix.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-0e8dcce2b46f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{HOME}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{HOME}/runs/detect/train/confusion_matrix.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1232\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/runs/detect/train/confusion_matrix.png'"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "Image(filename=f'{HOME}/runs/detect/train/confusion_matrix.png', width=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-urTWUkhRmn"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "Image(filename=f'{HOME}/runs/detect/train/results.png', width=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI4nADCCj3F5"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "Image(filename=f'{HOME}/runs/detect/train/val_batch0_pred.jpg', width=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ODk1VTlevxn"
      },
      "source": [
        "## Validate Custom Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpyuwrNlXc1P",
        "outputId": "e4532cf4-2b9f-42a3-d778-f8012feaec3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Ultralytics YOLOv8.0.196 üöÄ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 11127132 parameters, 0 gradients, 28.4 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/child-monitoring-11/valid/labels.cache... 67 images, 0 backgrounds, 0 corrupt: 100% 67/67 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:07<00:00,  1.43s/it]\n",
            "                   all         67        555      0.571      0.566      0.538      0.263\n",
            "             Caregiver         67         63      0.745      0.696      0.728      0.421\n",
            "                  Food         67        121       0.45      0.421       0.35      0.108\n",
            "                  Kids         67        309      0.726      0.841      0.848      0.446\n",
            "                  Toys         67         62      0.364      0.305      0.228     0.0786\n",
            "Speed: 13.1ms preprocess, 24.0ms inference, 0.0ms loss, 12.5ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
            "üí° Learn more at https://docs.ultralytics.com/modes/val\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "\n",
        "!yolo task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4eASbcWkQBq"
      },
      "source": [
        "## Inference with Custom Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjc1ctZykYuf",
        "outputId": "77532bd7-c5ea-4c5c-928d-723f6358684e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Ultralytics YOLOv8.0.196 üöÄ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 11127132 parameters, 0 gradients, 28.4 GFLOPs\n",
            "\n",
            "image 1/68 /content/datasets/child-monitoring-11/test/images/caregiver103_jpg.rf.83f2df4d0ae55cf6c80ec81a2c20e4fb.jpg: 800x800 10 Kidss, 1 Toys, 22.5ms\n",
            "image 2/68 /content/datasets/child-monitoring-11/test/images/caregiver104_jpg.rf.b82b2940b86b29eba22f64b788eab05e.jpg: 800x800 1 Caregiver, 6 Kidss, 22.5ms\n",
            "image 3/68 /content/datasets/child-monitoring-11/test/images/caregiver106_jpg.rf.138ae4208a5de894323a9a241902eb8e.jpg: 800x800 1 Caregiver, 15 Kidss, 22.4ms\n",
            "image 4/68 /content/datasets/child-monitoring-11/test/images/caregiver110_jpg.rf.525adf0b12e7508b9811112f3ecfbb6f.jpg: 800x800 1 Caregiver, 13 Kidss, 1 Toys, 22.5ms\n",
            "image 5/68 /content/datasets/child-monitoring-11/test/images/caregiver116_jpg.rf.07ded9e302ceb5badf3517a71bc5184c.jpg: 800x800 4 Caregivers, 24 Kidss, 22.4ms\n",
            "image 6/68 /content/datasets/child-monitoring-11/test/images/caregiver120_jpg.rf.14df4f78f4f636224f7ff578456a3cd1.jpg: 800x800 1 Food, 14 Kidss, 22.4ms\n",
            "image 7/68 /content/datasets/child-monitoring-11/test/images/caregiver128_jpg.rf.20f9b5ed0d71e95baf8bc4787fea04eb.jpg: 800x800 1 Caregiver, 1 Kids, 22.4ms\n",
            "image 8/68 /content/datasets/child-monitoring-11/test/images/caregiver12_png.rf.25fcc293afc9268ddc9fbc028b0de8e7.jpg: 800x800 2 Caregivers, 2 Kidss, 18.6ms\n",
            "image 9/68 /content/datasets/child-monitoring-11/test/images/caregiver134_jpg.rf.f7243ec08a742f31e68bf95d0b92e063.jpg: 800x800 10 Kidss, 18.6ms\n",
            "image 10/68 /content/datasets/child-monitoring-11/test/images/caregiver142_jpg.rf.32da055c7a8a2f4f96c88c70d57f0ca8.jpg: 800x800 2 Caregivers, 5 Kidss, 18.6ms\n",
            "image 11/68 /content/datasets/child-monitoring-11/test/images/caregiver146_jpg.rf.5e53b09041ee43e9dc9808a67f88ebd1.jpg: 800x800 6 Caregivers, 7 Kidss, 12 Toyss, 18.6ms\n",
            "image 12/68 /content/datasets/child-monitoring-11/test/images/caregiver153_jpg.rf.04ed479103784a9447ab2f4d08ac979a.jpg: 800x800 2 Caregivers, 21 Kidss, 18.6ms\n",
            "image 13/68 /content/datasets/child-monitoring-11/test/images/caregiver166_jpg.rf.f15badc78d2c2e82d4863a87cfe63939.jpg: 800x800 1 Caregiver, 1 Food, 8 Kidss, 1 Toys, 16.2ms\n",
            "image 14/68 /content/datasets/child-monitoring-11/test/images/caregiver175_jpg.rf.3651e2d8f61fc5f6a66f4b4ac1eafc2b.jpg: 800x800 9 Kidss, 16.3ms\n",
            "image 15/68 /content/datasets/child-monitoring-11/test/images/caregiver177_jpg.rf.d1fd475605558c6a18545f7b8eee2db7.jpg: 800x800 13 Kidss, 1 Toys, 16.2ms\n",
            "image 16/68 /content/datasets/child-monitoring-11/test/images/caregiver181_jpg.rf.ac5e88de8d6968dba2f5fc2bf7df4239.jpg: 800x800 1 Caregiver, 7 Kidss, 16.2ms\n",
            "image 17/68 /content/datasets/child-monitoring-11/test/images/caregiver198_jpg.rf.882e38fb30cff145508a2ba71b384646.jpg: 800x800 1 Caregiver, 9 Kidss, 24 Toyss, 16.2ms\n",
            "image 18/68 /content/datasets/child-monitoring-11/test/images/caregiver23_jpg.rf.0a4638b9b6482d152a571ea53ba5f960.jpg: 800x800 2 Caregivers, 14 Kidss, 15.6ms\n",
            "image 19/68 /content/datasets/child-monitoring-11/test/images/caregiver36_png.rf.3ea3a70103975d885d79cba4481beab0.jpg: 800x800 1 Caregiver, 7 Kidss, 2 Toyss, 15.6ms\n",
            "image 20/68 /content/datasets/child-monitoring-11/test/images/caregiver38_jpeg.rf.f726591db6195e3837438611d13546a9.jpg: 800x800 1 Caregiver, 6 Kidss, 15.6ms\n",
            "image 21/68 /content/datasets/child-monitoring-11/test/images/caregiver45_jpg.rf.c04809f081572c01418ef6e00b7c627c.jpg: 800x800 1 Caregiver, 14 Kidss, 15.6ms\n",
            "image 22/68 /content/datasets/child-monitoring-11/test/images/caregiver47_jpg.rf.33e176160844ad4482f258e6721dee4f.jpg: 800x800 4 Caregivers, 14 Kidss, 15.7ms\n",
            "image 23/68 /content/datasets/child-monitoring-11/test/images/caregiver4_jpg.rf.2e44e73bf26c557f8688e760feb0abae.jpg: 800x800 1 Caregiver, 10 Kidss, 15.6ms\n",
            "image 24/68 /content/datasets/child-monitoring-11/test/images/caregiver54_jpg.rf.93cb898a84edb0e3e5279dac810140af.jpg: 800x800 2 Caregivers, 1 Food, 8 Kidss, 15.6ms\n",
            "image 25/68 /content/datasets/child-monitoring-11/test/images/caregiver59_jpg.rf.6393228b9fee82f6ccda83c60e64aa3a.jpg: 800x800 3 Caregivers, 11 Kidss, 1 Toys, 15.6ms\n",
            "image 26/68 /content/datasets/child-monitoring-11/test/images/caregiver64_jpg.rf.e0c2e8acc9666262973426fa211e32fe.jpg: 800x800 2 Caregivers, 9 Kidss, 15.6ms\n",
            "image 27/68 /content/datasets/child-monitoring-11/test/images/caregiver66_jpg.rf.8c012e1e70b4c4bc5715458c99ed29e1.jpg: 800x800 1 Caregiver, 4 Kidss, 4 Toyss, 15.6ms\n",
            "image 28/68 /content/datasets/child-monitoring-11/test/images/caregiver70_jpg.rf.4a2775cecd267fb0d63a9d75d5002ed6.jpg: 800x800 1 Caregiver, 1 Food, 5 Kidss, 15.5ms\n",
            "image 29/68 /content/datasets/child-monitoring-11/test/images/caregiver7_png.rf.c8a4e731403b650f3ba65fa1821e5aea.jpg: 800x800 4 Kidss, 1 Toys, 15.4ms\n",
            "image 30/68 /content/datasets/child-monitoring-11/test/images/caregiver99_jpg.rf.047c879529b5dcb6b392d34daa7eadcf.jpg: 800x800 1 Caregiver, 2 Kidss, 15.4ms\n",
            "image 31/68 /content/datasets/child-monitoring-11/test/images/food_11_jpg.rf.fb30195e2e2d774e6538abbd9480e895.jpg: 800x800 4 Kidss, 15.5ms\n",
            "image 32/68 /content/datasets/child-monitoring-11/test/images/food_14343011-dcb3-4b86-8577-676751aea19f_jpeg.rf.2e8ba824b25ee64fb10de50800c8b49f.jpg: 800x800 18 Foods, 1 Toys, 15.4ms\n",
            "image 33/68 /content/datasets/child-monitoring-11/test/images/food_14_jpg.rf.3d9998fe46221cfec92405cf71e182ca.jpg: 800x800 (no detections), 15.2ms\n",
            "image 34/68 /content/datasets/child-monitoring-11/test/images/food_15_jpg.rf.d63a2231bdaa4d0c7fa48473f1764cb5.jpg: 800x800 3 Foods, 2 Kidss, 15.1ms\n",
            "image 35/68 /content/datasets/child-monitoring-11/test/images/food_171a9eee-3276-4ab9-b093-f6c486c6fda1_jpeg.rf.067b29f847adfc4fe3a83f639cd49b0d.jpg: 800x800 1 Food, 5 Kidss, 15.1ms\n",
            "image 36/68 /content/datasets/child-monitoring-11/test/images/food_18_jpg.rf.84882ebe808e338a02754f784c152175.jpg: 800x800 2 Foods, 4 Kidss, 15.1ms\n",
            "image 37/68 /content/datasets/child-monitoring-11/test/images/food_19_jpg.rf.710d8e40e8a1b2e98e74a47193259166.jpg: 800x800 1 Food, 3 Kidss, 15.1ms\n",
            "image 38/68 /content/datasets/child-monitoring-11/test/images/food_202446f1-a6cd-42d7-bdd8-9731be1d0d58_jpeg.rf.f1efe390661c4eb52c4b0d88f4805abd.jpg: 800x800 7 Foods, 7 Kidss, 15.1ms\n",
            "image 39/68 /content/datasets/child-monitoring-11/test/images/food_2314021a-b337-4b3d-a83a-417446297184_jpeg.rf.c41adead6a61938e0888ae5e608a91aa.jpg: 800x800 14 Foods, 1 Kids, 14.1ms\n",
            "image 40/68 /content/datasets/child-monitoring-11/test/images/food_24_jpg.rf.590f498bd50a30f4f1d070782b30827d.jpg: 800x800 1 Caregiver, 1 Food, 3 Kidss, 14.1ms\n",
            "image 41/68 /content/datasets/child-monitoring-11/test/images/food_27_jpg.rf.726144cebf12d06813bca3b3d05b5a77.jpg: 800x800 1 Caregiver, 4 Foods, 3 Kidss, 14.1ms\n",
            "image 42/68 /content/datasets/child-monitoring-11/test/images/food_53_jpg.rf.1f83103c8a5a084aeabc6d7b95965106.jpg: 800x800 1 Caregiver, 1 Food, 3 Kidss, 13.7ms\n",
            "image 43/68 /content/datasets/child-monitoring-11/test/images/food_53ea95d1-4906-4ad1-abd4-88891fe28b34_jpeg.rf.d33a1672e0f0bb2fa995c00d58482a3c.jpg: 800x800 1 Food, 3 Kidss, 13.8ms\n",
            "image 44/68 /content/datasets/child-monitoring-11/test/images/food_5ff07b8b-804f-4e38-9f8d-e148a13e4185_jpeg.rf.a071e35589c5d88cbf2fa600c2c25551.jpg: 800x800 1 Caregiver, 2 Foods, 6 Kidss, 13.7ms\n",
            "image 45/68 /content/datasets/child-monitoring-11/test/images/food_62261bca-5dde-4bd2-b482-a5a7b78e5959_jpeg.rf.7379ea475ec282055dc6d337e40b2615.jpg: 800x800 2 Foods, 3 Kidss, 13.8ms\n",
            "image 46/68 /content/datasets/child-monitoring-11/test/images/food_62_jpg.rf.884fa36928451ddc24393b985f6591f5.jpg: 800x800 2 Foods, 10 Kidss, 13.9ms\n",
            "image 47/68 /content/datasets/child-monitoring-11/test/images/food_64_jpg.rf.893980bf6c2fa0450bc352ae0894baf2.jpg: 800x800 5 Foods, 5 Kidss, 13.8ms\n",
            "image 48/68 /content/datasets/child-monitoring-11/test/images/food_70_jpg.rf.3b44a8636c4ffec8d0762729a88dfa34.jpg: 800x800 1 Caregiver, 3 Foods, 2 Kidss, 13.6ms\n",
            "image 49/68 /content/datasets/child-monitoring-11/test/images/food_7_jpg.rf.9549c9b53dfa39022477eac7cf39c468.jpg: 800x800 7 Foods, 4 Kidss, 13.7ms\n",
            "image 50/68 /content/datasets/child-monitoring-11/test/images/food_8_jpg.rf.a97e20df76694c2c7c61e707c67b0620.jpg: 800x800 1 Caregiver, 3 Foods, 4 Kidss, 15.1ms\n",
            "image 51/68 /content/datasets/child-monitoring-11/test/images/food_972fae87-e200-45aa-81f2-7847142e4c3b_jpeg.rf.63546491f8d29ac43299bfe8a98e248d.jpg: 800x800 5 Foods, 6 Kidss, 13.6ms\n",
            "image 52/68 /content/datasets/child-monitoring-11/test/images/food_Leonardo_Diffusion_XL_Photo_for_a_vibrant_and_authentic_kinder_0_jpg.rf.0a74067f4a7a86a420c5cc2e235bd1eb.jpg: 800x800 2 Foods, 4 Kidss, 13.6ms\n",
            "image 53/68 /content/datasets/child-monitoring-11/test/images/food_PhotoReal_Photo_for_a_vibrant_and_authentic_kindergarten_Japan_0_jpg.rf.bfdd170733f742bad5d03675053c16c6.jpg: 800x800 7 Foods, 3 Kidss, 13.6ms\n",
            "image 54/68 /content/datasets/child-monitoring-11/test/images/food_a3f53582-5452-4cd7-b30d-aa6037a890e7_jpeg.rf.175b9595f4a1643d9e3d9ec9c2a6877d.jpg: 800x800 2 Kidss, 13.6ms\n",
            "image 55/68 /content/datasets/child-monitoring-11/test/images/food_b81d0d57-2133-4cf9-9747-d53752d395db_jpeg.rf.532511ff9cab9edbb074140609261ee1.jpg: 800x800 2 Foods, 13.6ms\n",
            "image 56/68 /content/datasets/child-monitoring-11/test/images/food_b89ffc4d-31bb-4b36-a2fe-51585a7ebc53_jpeg.rf.68d084c3dea5a96f439e474a75fa5fca.jpg: 800x800 1 Caregiver, 2 Foods, 1 Kids, 13.6ms\n",
            "image 57/68 /content/datasets/child-monitoring-11/test/images/food_d6c8d4e7-2efe-4257-aa2e-f3751d2748f5_jpeg.rf.9716230541cdb2f32aedce92ffe3b3e9.jpg: 800x800 3 Foods, 5 Kidss, 13.6ms\n",
            "image 58/68 /content/datasets/child-monitoring-11/test/images/food_d81533d1-76fb-48a7-a479-4524b70f82ae_jpeg.rf.bac06b466b241ef15ccf35927070b463.jpg: 800x800 7 Foods, 1 Toys, 13.6ms\n",
            "image 59/68 /content/datasets/child-monitoring-11/test/images/food_f477f4bf-d3be-4b78-b04f-503351bff84d_jpeg.rf.f7e49b01deaab5f7cfdea203af38a4ac.jpg: 800x800 1 Caregiver, 2 Foods, 2 Kidss, 1 Toys, 13.6ms\n",
            "image 60/68 /content/datasets/child-monitoring-11/test/images/food_google_scrapped_image_111_jpg.rf.61c429d44e3095361d50dd1662e976ae.jpg: 800x800 2 Foods, 5 Kidss, 13.6ms\n",
            "image 61/68 /content/datasets/child-monitoring-11/test/images/food_google_scrapped_image_113_jpg.rf.e211a7ec8a134bf769707448525a2b3e.jpg: 800x800 2 Foods, 3 Kidss, 13.2ms\n",
            "image 62/68 /content/datasets/child-monitoring-11/test/images/food_google_scrapped_image_125_jpg.rf.2807ee233e28f098396072982f9226da.jpg: 800x800 3 Foods, 4 Kidss, 13.2ms\n",
            "image 63/68 /content/datasets/child-monitoring-11/test/images/food_google_scrapped_image_170_jpg.rf.f2bb8beefaf7377c8dba09ce88095a08.jpg: 800x800 1 Food, 3 Kidss, 13.2ms\n",
            "image 64/68 /content/datasets/child-monitoring-11/test/images/food_google_scrapped_image_90_jpg.rf.5bf8b95ff74ed51e99d3ca13f665e81c.jpg: 800x800 2 Foods, 5 Kidss, 1 Toys, 13.0ms\n",
            "image 65/68 /content/datasets/child-monitoring-11/test/images/food_google_scrapped_image_95_jpg.rf.8e131183b89a2649a50416181feda598.jpg: 800x800 9 Foods, 6 Kidss, 13.1ms\n",
            "image 66/68 /content/datasets/child-monitoring-11/test/images/food_l-2_jpg.rf.c5b172586ed44ccbc90cf9411a352361.jpg: 800x800 3 Foods, 6 Kidss, 13.1ms\n",
            "image 67/68 /content/datasets/child-monitoring-11/test/images/food_l-5_jpg.rf.e8fd1ca86a51e4c63fca1d3b3d2f1c9c.jpg: 800x800 7 Foods, 15 Kidss, 13.2ms\n",
            "image 68/68 /content/datasets/child-monitoring-11/test/images/food_l-6_jpg.rf.82676188246f62e991a6121d6af13166.jpg: 800x800 8 Foods, 6 Kidss, 13.2ms\n",
            "Speed: 3.7ms preprocess, 15.7ms inference, 9.7ms postprocess per image at shape (1, 3, 800, 800)\n",
            "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
            "üí° Learn more at https://docs.ultralytics.com/modes/predict\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "!yolo task=detect mode=predict model={HOME}/runs/detect/train/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEYIo95n-I0S"
      },
      "source": [
        "**NOTE:** Let's take a look at few results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jbVjEtPAkz3j"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for image_path in glob.glob(f'{HOME}/runs/detect/predict3/*.jpg')[:3]:\n",
        "      display(Image(filename=image_path, width=600))\n",
        "      print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0tsVilOCPyq"
      },
      "source": [
        "## Deploy model on Roboflow\n",
        "\n",
        "Once you have finished training your YOLOv8 model, you‚Äôll have a set of trained weights ready for use. These weights will be in the `/runs/detect/train/weights/best.pt` folder of your project. You can upload your model weights to Roboflow Deploy to use your trained weights on our infinitely scalable infrastructure.\n",
        "\n",
        "The `.deploy()` function in the [Roboflow pip package](https://docs.roboflow.com/python) now supports uploading YOLOv8 weights.\n",
        "\n",
        "To upload model weights, add the following code to the ‚ÄúInference with Custom Model‚Äù section in the aforementioned notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EhBAJ2gCPZh",
        "outputId": "259decf5-1c4e-4011-a208-a2498acc30ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the status of your deployment at: https://app.roboflow.com/roboflow-jvuqo/football-players-detection-3zvbc/1\n",
            "Share your model with the world at: https://universe.roboflow.com/roboflow-jvuqo/football-players-detection-3zvbc/1\n"
          ]
        }
      ],
      "source": [
        "project.version(dataset.version).deploy(model_type=\"yolov8\", model_path=f\"{HOME}/runs/detect/train/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5kOhjkmcV1l"
      },
      "outputs": [],
      "source": [
        "#While your deployment is processing, checkout the deployment docs to take your model to most destinations https://docs.roboflow.com/inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4bpUIibcV1l"
      },
      "outputs": [],
      "source": [
        "#Run inference on your model on a persistant, auto-scaling, cloud API\n",
        "\n",
        "#load model\n",
        "model = project.version(dataset.version).model\n",
        "\n",
        "#choose random test set image\n",
        "import os, random\n",
        "test_set_loc = dataset.location + \"/test/images/\"\n",
        "random_test_image = random.choice(os.listdir(test_set_loc))\n",
        "print(\"running inference on \" + random_test_image)\n",
        "\n",
        "pred = model.predict(test_set_loc + random_test_image, confidence=40, overlap=30).json()\n",
        "pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bMhLhBtxCyK"
      },
      "source": [
        "# Deploy Your Model to the Edge\n",
        "\n",
        "In addition to using the Roboflow hosted API for deployment, you can use [Roboflow Inference](https://inference.roboflow.com), an open source inference solution that has powered millions of API calls in production environments. Inference works with CPU and GPU, giving you immediate access to a range of devices, from the NVIDIA Jetson to TRT-compatible devices to ARM CPU devices.\n",
        "\n",
        "With Roboflow Inference, you can self-host and deploy your model on-device. You can deploy applications using the [Inference Docker containers](https://inference.roboflow.com/quickstart/docker/) or the pip package.\n",
        "\n",
        "For example, to install Inference on a device with an NVIDIA GPU, we can use:\n",
        "\n",
        "```\n",
        "docker pull roboflow/roboflow-inference-server-gpu\n",
        "```\n",
        "\n",
        "Then we can run inference via HTTP:\n",
        "\n",
        "```python\n",
        "import requests\n",
        "\n",
        "workspace_id = \"\"\n",
        "model_id = \"\"\n",
        "image_url = \"\"\n",
        "confidence = 0.75\n",
        "api_key = \"\"\n",
        "\n",
        "infer_payload = {\n",
        "    \"image\": {\n",
        "        \"type\": \"url\",\n",
        "        \"value\": image_url,\n",
        "    },\n",
        "    \"confidence\": confidence,\n",
        "    \"iou_threshold\": iou_thresh,\n",
        "    \"api_key\": api_key,\n",
        "}\n",
        "res = requests.post(\n",
        "    f\"http://localhost:9001/{workspace_id}/{model_id}\",\n",
        "    json=infer_object_detection_payload,\n",
        ")\n",
        "\n",
        "predictions = res.json()\n",
        "```\n",
        "\n",
        "Above, set your Roboflow workspace ID, model ID, and API key.\n",
        "\n",
        "- [Find your workspace and model ID](https://docs.roboflow.com/api-reference/workspace-and-project-ids?ref=blog.roboflow.com)\n",
        "- [Find your API key](https://docs.roboflow.com/api-reference/authentication?ref=blog.roboflow.com#retrieve-an-api-key)\n",
        "\n",
        "Also, set the URL of an image on which you want to run inference. This can be a local file.\n",
        "\n",
        "_To use your YOLOv5 model commercially with Inference, you will need a Roboflow Enterprise license, through which you gain a pass-through license for using YOLOv5. An enterprise license also grants you access to features like advanced device management, multi-model containers, auto-batch inference, and more._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OS5zYTXxCyK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovQgOj_xSNDg"
      },
      "source": [
        "## üèÜ Congratulations\n",
        "\n",
        "### Learning Resources\n",
        "\n",
        "Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:\n",
        "\n",
        "- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.\n",
        "- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.\n",
        "- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.\n",
        "- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.\n",
        "\n",
        "### Convert data formats\n",
        "\n",
        "Roboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.\n",
        "\n",
        "### Connect computer vision to your project logic\n",
        "\n",
        "[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}